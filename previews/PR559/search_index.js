var documenterSearchIndex = {"docs":
[{"location":"onemkl/#oneMKL-Integration","page":"oneMKL","title":"oneMKL Integration","text":"oneAPI.jl provides bindings to the Intel oneMKL library, enabling high-performance linear algebra operations on Intel GPUs.","category":"section"},{"location":"onemkl/#Dense-Linear-Algebra-(BLAS/LAPACK)","page":"oneMKL","title":"Dense Linear Algebra (BLAS/LAPACK)","text":"Standard BLAS and LAPACK operations are automatically accelerated when using oneArray.\n\nusing oneAPI, LinearAlgebra\n\nA = oneArray(rand(Float32, 100, 100))\nB = oneArray(rand(Float32, 100, 100))\n\n# Matrix multiplication (GEMM)\nC = A * B\n\n# Linear solve (AX = B)\nX = A \\ B","category":"section"},{"location":"onemkl/#Sparse-Linear-Algebra","page":"oneMKL","title":"Sparse Linear Algebra","text":"oneAPI.jl supports sparse matrix operations via oneMKL's sparse BLAS functionality. These integrate with Julia's SparseArrays standard library.\n\nusing oneAPI, oneAPI.oneMKL, SparseArrays, LinearAlgebra\n\n# Create a sparse matrix on CPU\nA = sprand(100, 100, 0.1)\n\n# Move to GPU (converts to oneMKL format)\ndA = oneMKL.oneSparseMatrixCSC(A)\n\n# Create a dense vector\nx = oneArray(rand(100))\n\n# Sparse matrix-vector multiplication\ny = dA * x\n\nNote that oneSparseMatrixCSC is available for Compressed Sparse Column format, which is the standard in Julia.","category":"section"},{"location":"onemkl/#FFTs","page":"oneMKL","title":"FFTs","text":"Fast Fourier Transforms are supported through AbstractFFTs.jl interface integration with oneMKL DFTs.\n\nusing oneAPI, FFTW\n\na = oneArray(rand(ComplexF32, 1024))\n\n# Forward FFT\nb = fft(a)\n\n# Inverse FFT\nc = ifft(b)","category":"section"},{"location":"usage/performance/#Performance-Guide","page":"Performance Guide","title":"Performance Guide","text":"This guide provides tips and techniques for optimizing oneAPI.jl applications.","category":"section"},{"location":"usage/performance/#Quick-Wins","page":"Performance Guide","title":"Quick Wins","text":"","category":"section"},{"location":"usage/performance/#1.-Use-Device-Memory","page":"Performance Guide","title":"1. Use Device Memory","text":"Device memory is fastest for GPU operations:\n\n# ✅ Good: Device memory (default)\na = oneArray{Float32}(undef, 1000)\n\n# ❌ Slower: Shared memory (unless CPU access is needed)\na = oneArray{Float32,1,oneL0.SharedBuffer}(undef, 1000)","category":"section"},{"location":"usage/performance/#2.-Minimize-Data-Transfers","page":"Performance Guide","title":"2. Minimize Data Transfers","text":"Keep data on GPU between operations:\n\n# ❌ Bad: Unnecessary transfers\nfor i in 1:100\n    cpu_data = Array(gpu_array)  # GPU → CPU\n    cpu_data .+= 1\n    gpu_array = oneArray(cpu_data)  # CPU → GPU\nend\n\n# ✅ Good: Keep data on GPU\nfor i in 1:100\n    gpu_array .+= 1  # All on GPU\nend","category":"section"},{"location":"usage/performance/#3.-Use-Fused-Operations","page":"Performance Guide","title":"3. Use Fused Operations","text":"Broadcasting automatically fuses operations:\n\n# ❌ Slower: Multiple kernel launches\na = oneArray(rand(Float32, 1000))\nb = sin.(a)\nc = b .+ 1.0f0\nd = c .* 2.0f0\n\n# ✅ Faster: Single fused kernel\nd = 2.0f0 .* (sin.(a) .+ 1.0f0)","category":"section"},{"location":"usage/performance/#4.-Specify-Float32","page":"Performance Guide","title":"4. Specify Float32","text":"GPUs are typically optimized for single precision:\n\n# ❌ Slower: Float64 (if not needed)\na = oneArray(rand(Float64, 1000))\n\n# ✅ Faster: Float32\na = oneArray(rand(Float32, 1000))","category":"section"},{"location":"usage/performance/#Kernel-Optimization","page":"Performance Guide","title":"Kernel Optimization","text":"","category":"section"},{"location":"usage/performance/#Launch-Configuration","page":"Performance Guide","title":"Launch Configuration","text":"Choose appropriate workgroup sizes:\n\n# Typical good workgroup sizes\nitems = 256   # Common choice, adjust based on hardware\nitems = 128   # Try smaller if using lots of local memory\nitems = 512   # Try larger for simple kernels\n\n# Calculate groups\nN = length(array)\ngroups = cld(N, items)  # Ceiling division\n\n@oneapi groups=groups items=items kernel(array)","category":"section"},{"location":"usage/performance/#Memory-Access-Patterns","page":"Performance Guide","title":"Memory Access Patterns","text":"Coalesced memory access is crucial for performance:\n\n# ✅ Good: Coalesced access (consecutive threads access consecutive memory)\nfunction good_kernel!(output, input)\n    i = get_global_id()\n    @inbounds output[i] = input[i] + 1.0f0\n    return\nend\n\n# ❌ Bad: Strided access (cache inefficient)\nfunction bad_kernel!(output, input, stride)\n    i = get_global_id()\n    @inbounds output[i] = input[i * stride] + 1.0f0\n    return\nend","category":"section"},{"location":"usage/performance/#Use-Local-Memory","page":"Performance Guide","title":"Use Local Memory","text":"Local memory is faster than global memory for data reuse:\n\nfunction optimized_reduction!(result, input)\n    local_id = get_local_id()\n    local_size = get_local_size()\n    group_id = get_group_id()\n\n    # Allocate local memory\n    local_mem = oneLocalArray(Float32, 256)\n\n    # Load global → local (coalesced)\n    global_id = get_global_id()\n    @inbounds local_mem[local_id] = input[global_id]\n    barrier()\n\n    # Reduce in local memory (much faster)\n    stride = local_size ÷ 2\n    while stride > 0\n        if local_id <= stride\n            @inbounds local_mem[local_id] += local_mem[local_id + stride]\n        end\n        barrier()\n        stride ÷= 2\n    end\n\n    # Write result\n    if local_id == 1\n        @inbounds result[group_id] = local_mem[1]\n    end\n    return\nend","category":"section"},{"location":"usage/performance/#Minimize-Barriers","page":"Performance Guide","title":"Minimize Barriers","text":"Barriers have overhead:\n\n# ❌ Bad: Unnecessary barriers\nfunction wasteful_kernel!(a)\n    i = get_local_id()\n    a[i] += 1\n    barrier()  # Not needed if no data sharing\n    a[i] *= 2\n    barrier()  # Not needed\n    return\nend\n\n# ✅ Good: Barriers only when needed\nfunction efficient_kernel!(a, shared)\n    i = get_local_id()\n\n    # Load to shared memory\n    shared[i] = a[i]\n    barrier()  # Needed: ensure all loads complete\n\n    # Use shared data\n    result = shared[i] + shared[i+1]\n    a[i] = result\n    return\nend","category":"section"},{"location":"usage/performance/#Avoid-Divergence","page":"Performance Guide","title":"Avoid Divergence","text":"Minimize thread divergence (different execution paths):\n\n# ❌ Bad: High divergence\nfunction divergent_kernel!(a)\n    i = get_global_id()\n    if i % 32 == 0\n        # Only 1 in 32 threads executes this\n        @inbounds a[i] = expensive_computation(a[i])\n    else\n        @inbounds a[i] += 1.0f0\n    end\n    return\nend\n\n# ✅ Better: Separate into different kernels\nfunction uniform_kernel!(a)\n    i = get_global_id()\n    @inbounds a[i] += 1.0f0\n    return\nend\n\nfunction sparse_kernel!(a, indices)\n    i = get_global_id()\n    if i <= length(indices)\n        idx = indices[i]\n        @inbounds a[idx] = expensive_computation(a[idx])\n    end\n    return\nend","category":"section"},{"location":"usage/performance/#Type-Stability","page":"Performance Guide","title":"Type Stability","text":"Type instability severely hurts performance:\n\n# ❌ Bad: Type unstable\nfunction unstable_kernel!(output, input, flag)\n    i = get_global_id()\n    if flag\n        value = input[i]  # Float32\n    else\n        value = 0         # Int\n    end\n    output[i] = value * 2  # Type uncertain!\n    return\nend\n\n# ✅ Good: Type stable\nfunction stable_kernel!(output, input, flag)\n    i = get_global_id()\n    if flag\n        value = input[i]  # Float32\n    else\n        value = 0.0f0     # Float32\n    end\n    output[i] = value * 2.0f0  # All Float32!\n    return\nend\n\n# Check type stability\n@device_code_warntype @oneapi groups=1 items=10 stable_kernel!(output, input, true)","category":"section"},{"location":"usage/performance/#Algorithmic-Optimization","page":"Performance Guide","title":"Algorithmic Optimization","text":"","category":"section"},{"location":"usage/performance/#Use-Library-Functions","page":"Performance Guide","title":"Use Library Functions","text":"Leverage optimized library implementations:\n\nusing oneAPI, LinearAlgebra\n\n# ✅ Good: Use oneMKL through LinearAlgebra\nA = oneArray(rand(Float32, 1000, 1000))\nB = oneArray(rand(Float32, 1000, 1000))\nC = A * B  # Uses optimized oneMKL\n\n# ❌ Bad: Write your own matrix multiplication\n# (unless you have a very specific use case)","category":"section"},{"location":"usage/performance/#Choose-Right-Algorithm","page":"Performance Guide","title":"Choose Right Algorithm","text":"Some algorithms parallelize better than others:\n\n# ❌ Sequential algorithm\nfunction sequential_sum(arr)\n    sum = 0.0f0\n    for x in arr\n        sum += x\n    end\n    return sum\nend\n\n# ✅ Parallel reduction\nresult = sum(oneArray(data))  # Optimized parallel reduction","category":"section"},{"location":"usage/performance/#Benchmarking","page":"Performance Guide","title":"Benchmarking","text":"","category":"section"},{"location":"usage/performance/#Basic-Timing","page":"Performance Guide","title":"Basic Timing","text":"using BenchmarkTools, oneAPI\n\na = oneArray(rand(Float32, 1000))\nb = oneArray(rand(Float32, 1000))\n\n# Warmup\nc = a .+ b\nsynchronize()\n\n# Benchmark\n@benchmark begin\n    c = $a .+ $b\n    synchronize()\nend","category":"section"},{"location":"usage/performance/#Accurate-GPU-Timing","page":"Performance Guide","title":"Accurate GPU Timing","text":"Always synchronize before timing:\n\nusing oneAPI\n\na = oneArray(rand(Float32, 1_000_000))\n\n# ❌ Wrong: Doesn't wait for GPU\n@time a .+= 1  # Only measures kernel launch overhead\n\n# ✅ Correct: Wait for GPU to finish\n@time begin\n    a .+= 1\n    synchronize()\nend","category":"section"},{"location":"usage/performance/#Profiling-with-Time","page":"Performance Guide","title":"Profiling with Time","text":"function profile_operation(a, b)\n    # Warmup\n    c = a .+ b\n    synchronize()\n\n    # Time kernel launch\n    t1 = time()\n    c = a .+ b\n    t2 = time()\n    launch_time = t2 - t1\n\n    # Time including synchronization\n    synchronize()\n    t3 = time()\n    total_time = t3 - t1\n\n    println(\"Launch: \", launch_time * 1000, \" ms\")\n    println(\"Total:  \", total_time * 1000, \" ms\")\n    println(\"Actual: \", (total_time - launch_time) * 1000, \" ms\")\nend\n\na = oneArray(rand(Float32, 10_000_000))\nb = oneArray(rand(Float32, 10_000_000))\nprofile_operation(a, b)","category":"section"},{"location":"usage/performance/#Memory-Bandwidth","page":"Performance Guide","title":"Memory Bandwidth","text":"","category":"section"},{"location":"usage/performance/#Theoretical-Peak","page":"Performance Guide","title":"Theoretical Peak","text":"Calculate theoretical bandwidth:\n\n# Example: Intel Iris Xe Graphics\n# 96 execution units, 1.35 GHz\n# Memory bandwidth: ~68 GB/s\n\n# Your kernel processes N Float32 values\nN = 10_000_000\nbytes_transferred = N * sizeof(Float32) * 2  # Read + Write\n\n# Measure time\nt = @elapsed begin\n    a .+= b\n    synchronize()\nend\n\nbandwidth_achieved = bytes_transferred / t / 1e9  # GB/s\nprintln(\"Bandwidth: \", bandwidth_achieved, \" GB/s\")","category":"section"},{"location":"usage/performance/#Improving-Bandwidth-Utilization","page":"Performance Guide","title":"Improving Bandwidth Utilization","text":"# ✅ Good: Single pass with fusion\nresult = @. a + b * c - d / e  # One pass over data\n\n# ❌ Bad: Multiple passes\nresult = a .+ b\nresult = result .* c\nresult = result .- d\nresult = result ./ e\n# Four separate passes over data!","category":"section"},{"location":"usage/performance/#Common-Performance-Issues","page":"Performance Guide","title":"Common Performance Issues","text":"","category":"section"},{"location":"usage/performance/#Issue-1:-Too-Many-Small-Kernels","page":"Performance Guide","title":"Issue 1: Too Many Small Kernels","text":"# ❌ Bad: Many small kernel launches\nfor i in 1:100\n    a .+= 1  # 100 kernel launches!\nend\n\n# ✅ Good: Single kernel or batching\na .+= 100  # Single operation","category":"section"},{"location":"usage/performance/#Issue-2:-Unnecessary-Allocations","page":"Performance Guide","title":"Issue 2: Unnecessary Allocations","text":"# ❌ Bad: Allocates temporary\nc = a .+ b  # Allocates new array\n\n# ✅ Good: In-place operation\nc = similar(a)\nc .= a .+ b  # Uses pre-allocated array","category":"section"},{"location":"usage/performance/#Issue-3:-Wrong-Number-Type","page":"Performance Guide","title":"Issue 3: Wrong Number Type","text":"# ❌ Bad: Mixed types\na = oneArray(rand(Float32, 1000))\nb = a .+ 1.0  # Float64 constant!\n\n# ✅ Good: Matching types\nb = a .+ 1.0f0  # Float32 constant","category":"section"},{"location":"usage/performance/#Performance-Checklist","page":"Performance Guide","title":"Performance Checklist","text":"[ ] Using device memory (not shared unless necessary)\n[ ] Minimizing CPU-GPU transfers\n[ ] Using Float32 (unless Float64 required)\n[ ] Fusing operations with broadcasting\n[ ] Type-stable kernels (@device_code_warntype)\n[ ] Appropriate workgroup sizes\n[ ] Coalesced memory access\n[ ] Minimal thread divergence\n[ ] Leveraging local memory for reuse\n[ ] Using library functions when available\n[ ] Synchronizing before timing\n[ ] Avoiding unnecessary allocations","category":"section"},{"location":"usage/performance/#Hardware-Specific-Tuning","page":"Performance Guide","title":"Hardware-Specific Tuning","text":"Different Intel GPUs have different characteristics:\n\nusing oneAPI.oneL0\n\ndev = device()\nprops = properties(dev)\ncompute_props = compute_properties(dev)\n\nprintln(\"Device: \", props.name)\nprintln(\"EU count: \", compute_props.numEUsPerSubslice *\n                       compute_props.numSubslicesPerSlice *\n                       compute_props.numSlices)\nprintln(\"Max workgroup size: \", compute_props.maxTotalGroupSize)\nprintln(\"Max local memory: \", compute_props.maxSharedLocalMemory, \" bytes\")\n\n# Adjust your code based on these properties","category":"section"},{"location":"usage/performance/#Advanced:-Async-Operations","page":"Performance Guide","title":"Advanced: Async Operations","text":"For overlapping compute and transfers (advanced users):\n\nusing oneAPI.oneL0\n\nctx = context()\ndev = device()\n\n# Create multiple queues for async operations\nqueue1 = ZeCommandQueue(ctx, dev)\nqueue2 = ZeCommandQueue(ctx, dev)\n\n# Launch kernel on queue1\nexecute!(queue1) do list\n    # ... kernel launch ...\nend\n\n# Overlap with transfer on queue2\nexecute!(queue2) do list\n    append_copy!(list, dst, src, size)\nend\n\n# Synchronize both\nsynchronize(queue1)\nsynchronize(queue2)","category":"section"},{"location":"usage/performance/#Further-Resources","page":"Performance Guide","title":"Further Resources","text":"Intel GPU Architecture\noneAPI Programming Guide\nLevel Zero Specification","category":"section"},{"location":"installation/#Installation","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"installation/#Requirements","page":"Installation","title":"Requirements","text":"oneAPI.jl requires:\n\nJulia: 1.10 or higher\nOS: Linux (recommended) or Windows (experimental via WSL2)\nHardware: Intel Gen9 graphics or newer. For Intel Arc GPUs (A580, A750, A770, etc), Linux 6.2+ is required.","category":"section"},{"location":"installation/#Installing-oneAPI.jl","page":"Installation","title":"Installing oneAPI.jl","text":"You can install oneAPI.jl using the Julia package manager:\n\npkg> add oneAPI\n\nThis will automatically download the necessary binary dependencies, including:\n\noneAPI loader\nSPIR-V tools\nIntel Compute Runtime (if compatible hardware is found)","category":"section"},{"location":"installation/#Verifying-Installation","page":"Installation","title":"Verifying Installation","text":"After installation, you can verify that oneAPI.jl is working correctly and detecting your hardware:\n\njulia> using oneAPI\njulia> oneAPI.versioninfo()\n\nThe output should list the binary dependencies, toolchain versions, available drivers, and devices.","category":"section"},{"location":"installation/#Troubleshooting-Drivers","page":"Installation","title":"Troubleshooting Drivers","text":"If no drivers or devices are detected, ensure that you have the correct Intel graphics drivers installed for your system.\n\nOn Linux, check if libze_intel_gpu.so or similar libraries are available.\nOn Windows (WSL2), ensure you have the latest Intel graphics drivers installed on the host Windows system and that WSL2 is configured to access the GPU.\n\nYou can explicitly select drivers and devices if multiple are available:\n\njulia> drivers()\njulia> devices()\njulia> device!(1) # Select the first available device","category":"section"},{"location":"installation/#Using-System-Libraries-(Advanced)","page":"Installation","title":"Using System Libraries (Advanced)","text":"warning: Warning\nUsing system libraries instead of the provided artifacts is not recommended for most users. Only use this approach if you have specialized requirements or custom Intel binaries.\n\nBy default, oneAPI.jl uses pre-built binary artifacts (JLLs) for the Intel Compute Runtime, oneAPI loader, and related libraries. However, you may need to use system-installed libraries in certain situations:\n\nCustom or newer Intel graphics drivers\nSpecialized hardware configurations\nDevelopment or debugging of the runtime stack\nSystems where the artifacts are incompatible","category":"section"},{"location":"installation/#Configuration-Script","page":"Installation","title":"Configuration Script","text":"oneAPI.jl provides a helper script to discover and configure system libraries. From the Julia REPL:\n\njulia> include(joinpath(pkgdir(oneAPI), \"res\", \"local.jl\"))\n\nThis script will:\n\nSearch for Intel libraries on your system:\nIntel Graphics Compiler (IGC): libigc, libiga64, libigdfcl, libopencl-clang\nGraphics Memory Management Library: libigdgmm\nIntel Compute Runtime (NEO): libze_intel_gpu, libigdrcl\noneAPI Level Zero Loader: libze_loader, libze_validation_layer\nGenerate preferences in LocalPreferences.toml that override the artifact paths","category":"section"},{"location":"installation/#Manual-Configuration","page":"Installation","title":"Manual Configuration","text":"You can also manually set preferences to use specific library paths. Create or edit LocalPreferences.toml in your project or global environment:\n\n[NEO_jll]\nlibze_intel_gpu_path = \"/usr/lib/x86_64-linux-gnu/libze_intel_gpu.so.1\"\nlibigdrcl_path = \"/usr/lib/x86_64-linux-gnu/intel-opencl/libigdrcl.so\"\n\n[libigc_jll]\nlibigc_path = \"/usr/lib/x86_64-linux-gnu/libigc.so\"\nlibigdfcl_path = \"/usr/lib/x86_64-linux-gnu/libigdfcl.so\"\n\n[gmmlib_jll]\nlibigdgmm_path = \"/usr/lib/x86_64-linux-gnu/libigdgmm.so\"\n\n[oneAPI_Level_Zero_Loader_jll]\nlibze_loader_path = \"/usr/lib/x86_64-linux-gnu/libze_loader.so\"","category":"section"},{"location":"installation/#Reverting-to-Artifacts","page":"Installation","title":"Reverting to Artifacts","text":"To revert to the default artifact binaries, simply delete the oneAPI-related entries from LocalPreferences.toml (or delete the entire file if it only contains these preferences).","category":"section"},{"location":"installation/#Common-Locations","page":"Installation","title":"Common Locations","text":"System libraries are typically installed in:\n\nUbuntu/Debian:\n\n/usr/lib/x86_64-linux-gnu/\n/usr/lib/x86_64-linux-gnu/intel-opencl/\n\nFedora/RHEL:\n\n/usr/lib64/\n/usr/lib64/intel-opencl/\n\nCustom Intel oneAPI installation:\n\n/opt/intel/oneapi/compiler/latest/linux/lib/\n/opt/intel/oneapi/compiler/latest/linux/lib/x64/","category":"section"},{"location":"installation/#Verifying-System-Library-Configuration","page":"Installation","title":"Verifying System Library Configuration","text":"After configuring system libraries, restart Julia and verify the configuration:\n\njulia> using oneAPI\njulia> oneAPI.versioninfo()\n\nCheck that the reported library paths match your system libraries. If issues arise, examine the LocalPreferences.toml file and ensure all paths are correct and the libraries are compatible with each other.","category":"section"},{"location":"level_zero/#Level-Zero-Interface","page":"Level Zero (oneL0)","title":"Level Zero Interface","text":"The oneL0 submodule provides low-level access to the Level Zero API, which gives you fine-grained control over the hardware.","category":"section"},{"location":"level_zero/#Drivers-and-Devices","page":"Level Zero (oneL0)","title":"Drivers and Devices","text":"You can enumerate available drivers and devices:\n\nusing oneAPI.oneL0\n\n# Get available drivers\ndrvs = drivers()\n\n# Get devices for a driver\ndevs = devices(first(drvs))\n\n# Inspect device properties\nprops = compute_properties(first(devs))\nprintln(\"Max workgroup size: \", props.maxTotalGroupSize)","category":"section"},{"location":"level_zero/#Contexts-and-Queues","page":"Level Zero (oneL0)","title":"Contexts and Queues","text":"Manage contexts and command queues for executing operations:\n\n# Create a context\nctx = ZeContext(first(drvs))\n\n# Create a command queue\nqueue = ZeCommandQueue(ctx, first(devs))\n\n# Execute a command list\nexecute!(queue) do list\n    append_barrier!(list)\nend","category":"section"},{"location":"level_zero/#Memory-Operations","page":"Level Zero (oneL0)","title":"Memory Operations","text":"You can perform low-level memory operations using command lists:\n\nexecute!(queue) do list\n    append_copy!(list, dst_ptr, src_ptr, size)\nend","category":"section"},{"location":"api/arrays/#Array-Operations","page":"Array Operations","title":"Array Operations","text":"This page documents the array types and operations provided by oneAPI.jl.","category":"section"},{"location":"api/arrays/#Array-Types","page":"Array Operations","title":"Array Types","text":"","category":"section"},{"location":"api/arrays/#Host-Side-Arrays","page":"Array Operations","title":"Host-Side Arrays","text":"","category":"section"},{"location":"api/arrays/#oneArray{T,N,B}","page":"Array Operations","title":"oneArray{T,N,B}","text":"N-dimensional dense array type for Intel GPU programming using oneAPI and Level Zero.\n\nType Parameters:\n\nT: Element type (must be stored inline, no isbits-unions)\nN: Number of dimensions\nB: Buffer type, one of:\noneL0.DeviceBuffer: GPU device memory (default, not CPU-accessible)\noneL0.SharedBuffer: Unified shared memory (CPU and GPU accessible)\noneL0.HostBuffer: Pinned host memory (CPU-accessible, GPU-visible)\n\nType Aliases:\n\noneVector{T} = oneArray{T,1} - 1D array\noneMatrix{T} = oneArray{T,2} - 2D array\noneVecOrMat{T} = Union{oneVector{T}, oneMatrix{T}} - 1D or 2D array","category":"section"},{"location":"api/arrays/#Device-Side-Arrays","page":"Array Operations","title":"Device-Side Arrays","text":"","category":"section"},{"location":"api/arrays/#oneDeviceArray{T,N,A}","page":"Array Operations","title":"oneDeviceArray{T,N,A}","text":"Device-side array type for use within GPU kernels. This type represents a view of GPU memory accessible within kernel code. Unlike oneArray which is used on the host, oneDeviceArray is designed for device-side operations and cannot be directly constructed on the host.\n\nType Parameters:\n\nT: Element type\nN: Number of dimensions\nA: Address space (typically AS.CrossWorkgroup for global memory)\n\nType Aliases:\n\noneDeviceVector = oneDeviceArray{T,1} - 1D device array\noneDeviceMatrix = oneDeviceArray{T,2} - 2D device array","category":"section"},{"location":"api/arrays/#oneLocalArray(::Type{T},-dims)","page":"Array Operations","title":"oneLocalArray(::Type{T}, dims)","text":"Allocate local (workgroup-shared) memory within a GPU kernel. Local memory is shared among all work-items in a workgroup and provides faster access than global memory.","category":"section"},{"location":"api/arrays/#Memory-Type-Queries","page":"Array Operations","title":"Memory Type Queries","text":"","category":"section"},{"location":"api/arrays/#is_device(a::oneArray)-Bool","page":"Array Operations","title":"is_device(a::oneArray) -> Bool","text":"Check if the array is stored in device memory (not directly CPU-accessible).","category":"section"},{"location":"api/arrays/#is_shared(a::oneArray)-Bool","page":"Array Operations","title":"is_shared(a::oneArray) -> Bool","text":"Check if the array is stored in shared (unified) memory, accessible from both CPU and GPU.","category":"section"},{"location":"api/arrays/#is_host(a::oneArray)-Bool","page":"Array Operations","title":"is_host(a::oneArray) -> Bool","text":"Check if the array is stored in pinned host memory, which resides on the CPU but is visible to the GPU.","category":"section"},{"location":"api/arrays/#Array-Construction","page":"Array Operations","title":"Array Construction","text":"oneArray supports multiple construction patterns similar to standard Julia arrays:\n\nusing oneAPI\n\n# Uninitialized arrays\na = oneArray{Float32}(undef, 100)\nb = oneArray{Float32,2}(undef, 10, 10)\n\n# Specify memory type\nc = oneArray{Float32,1,oneL0.SharedBuffer}(undef, 100)  # Shared memory\nd = oneArray{Float32,1,oneL0.HostBuffer}(undef, 100)    # Host memory\n\n# From existing arrays\ne = oneArray(rand(Float32, 100))\nf = oneArray([1, 2, 3, 4])\n\n# Using zeros/ones/rand\ng = oneAPI.zeros(Float32, 100)\nh = oneAPI.ones(Float32, 100)\ni = oneAPI.rand(Float32, 100)\n\n# Do-block for automatic cleanup\nresult = oneArray{Float32}(100) do arr\n    arr .= 1.0f0\n    sum(arr)  # Returns result, arr is freed automatically\nend","category":"section"},{"location":"api/arrays/#Array-Operations-2","page":"Array Operations","title":"Array Operations","text":"oneArray implements the full AbstractArray interface and supports:","category":"section"},{"location":"api/arrays/#Broadcasting","page":"Array Operations","title":"Broadcasting","text":"a = oneArray(rand(Float32, 100))\nb = oneArray(rand(Float32, 100))\n\nc = a .+ b          # Element-wise addition\nd = a .* 2.0f0      # Scalar multiplication\ne = sin.(a)         # Unary operations\nf = a .+ b .* c     # Fused operations","category":"section"},{"location":"api/arrays/#Reductions","page":"Array Operations","title":"Reductions","text":"a = oneArray(rand(Float32, 100))\n\ns = sum(a)          # Sum\np = prod(a)         # Product\nm = maximum(a)      # Maximum\nn = minimum(a)      # Minimum\nμ = mean(a)         # Mean (requires Statistics)","category":"section"},{"location":"api/arrays/#Mapping","page":"Array Operations","title":"Mapping","text":"a = oneArray(rand(Float32, 100))\n\nb = map(x -> x^2, a)        # Apply function\nc = map(+, a, b)            # Binary operation","category":"section"},{"location":"api/arrays/#Accumulation","page":"Array Operations","title":"Accumulation","text":"a = oneArray([1, 2, 3, 4])\n\nb = cumsum(a)       # Cumulative sum: [1, 3, 6, 10]\nc = cumprod(a)      # Cumulative product: [1, 2, 6, 24]","category":"section"},{"location":"api/arrays/#Finding-Elements","page":"Array Operations","title":"Finding Elements","text":"a = oneArray([1.0f0, -2.0f0, 3.0f0, -4.0f0])\n\nindices = findall(x -> x > 0, a)  # Indices of positive elements","category":"section"},{"location":"api/arrays/#Random-Number-Generation","page":"Array Operations","title":"Random Number Generation","text":"using oneAPI, Random\n\n# Uniform distribution\na = oneAPI.rand(Float32, 100)\nb = oneAPI.rand(Float32, 10, 10)\n\n# Normal distribution\nc = oneAPI.randn(Float32, 100)\n\n# With seed\nRandom.seed!(1234)\nd = oneAPI.rand(Float32, 100)","category":"section"},{"location":"api/arrays/#Data-Transfer","page":"Array Operations","title":"Data Transfer","text":"","category":"section"},{"location":"api/arrays/#CPU-to-GPU","page":"Array Operations","title":"CPU to GPU","text":"# Using constructor\nh_array = rand(Float32, 100)\nd_array = oneArray(h_array)\n\n# Using copyto!\nd_array = oneArray{Float32}(undef, 100)\ncopyto!(d_array, h_array)","category":"section"},{"location":"api/arrays/#GPU-to-CPU","page":"Array Operations","title":"GPU to CPU","text":"# Using Array constructor\nh_array = Array(d_array)\n\n# Using copyto!\nh_array = Vector{Float32}(undef, 100)\ncopyto!(h_array, d_array)","category":"section"},{"location":"api/arrays/#GPU-to-GPU","page":"Array Operations","title":"GPU to GPU","text":"d_array1 = oneArray(rand(Float32, 100))\nd_array2 = similar(d_array1)\ncopyto!(d_array2, d_array1)","category":"section"},{"location":"api/arrays/#Memory-Types-Comparison","page":"Array Operations","title":"Memory Types Comparison","text":"Memory Type CPU Access GPU Access Performance Use Case\nDevice (default) ❌ No ✅ Fast Fastest GPU computations\nShared ✅ Yes ✅ Good Good CPU-GPU data sharing\nHost ✅ Yes ✅ Slower Moderate Staging, pinned buffers\n\n# Device memory (default, fastest for GPU)\na = oneArray{Float32}(undef, 100)\n\n# Shared memory (CPU and GPU accessible)\nb = oneArray{Float32,1,oneL0.SharedBuffer}(undef, 100)\n\n# Host memory (CPU memory visible to GPU)\nc = oneArray{Float32,1,oneL0.HostBuffer}(undef, 100)\n\n# Query memory type\nis_device(a)  # true\nis_shared(b)  # true\nis_host(c)    # true","category":"section"},{"location":"api/arrays/#Views-and-Slicing","page":"Array Operations","title":"Views and Slicing","text":"oneArray supports array views for efficient sub-array operations without copying:\n\na = oneArray(rand(Float32, 100))\n\n# Create a view\nv = view(a, 1:50)\nv .= 0.0f0  # Modifies first 50 elements of a\n\n# Slicing returns a view\ns = a[1:50]  # This is a view, not a copy","category":"section"},{"location":"api/arrays/#Reshaping","page":"Array Operations","title":"Reshaping","text":"a = oneArray(rand(Float32, 100))\n\n# Reshape to 2D\nb = reshape(a, 10, 10)\n\n# Flatten\nc = vec(b)  # Returns 1D view","category":"section"},{"location":"api/arrays/#Advanced:-Custom-Array-Wrappers","page":"Array Operations","title":"Advanced: Custom Array Wrappers","text":"For advanced use cases, oneAPI.jl provides type aliases for array wrappers:\n\noneDenseArray: Dense contiguous arrays\noneStridedArray: Arrays with arbitrary strides (including views)\noneWrappedArray: Any array backed by a oneArray\n\nThese are useful for writing functions that accept various array types:\n\nfunction my_kernel!(a::oneStridedArray{Float32})\n    # Accepts oneArray and views\n    a .+= 1.0f0\nend","category":"section"},{"location":"api/context/#Context-and-Device-Management","page":"Context & Device Management","title":"Context and Device Management","text":"This page documents the API for managing Level Zero drivers, devices, and contexts in oneAPI.jl.","category":"section"},{"location":"api/context/#Overview","page":"Context & Device Management","title":"Overview","text":"oneAPI.jl uses task-local state to manage GPU resources. This allows different Julia tasks to work with different drivers, devices, or contexts without interfering with each other.\n\nThe typical hierarchy is:\n\nDriver: Represents a Level Zero driver (usually one per GPU vendor/installation)\nDevice: Represents a physical GPU device\nContext: Manages resources like memory allocations and command queues","category":"section"},{"location":"api/context/#Driver-Management","page":"Context & Device Management","title":"Driver Management","text":"","category":"section"},{"location":"api/context/#driver()-ZeDriver","page":"Context & Device Management","title":"driver() -> ZeDriver","text":"Get the current Level Zero driver for the calling task. If no driver has been explicitly set with driver!, returns the first available driver. The driver selection is task-local.","category":"section"},{"location":"api/context/#driver!(drv::ZeDriver)","page":"Context & Device Management","title":"driver!(drv::ZeDriver)","text":"Set the current Level Zero driver for the calling task. This also clears the current device selection, as devices are associated with specific drivers.","category":"section"},{"location":"api/context/#drivers()-Vector{ZeDriver}","page":"Context & Device Management","title":"drivers() -> Vector{ZeDriver}","text":"Return a list of all available Level Zero drivers.","category":"section"},{"location":"api/context/#Device-Management","page":"Context & Device Management","title":"Device Management","text":"","category":"section"},{"location":"api/context/#device()-ZeDevice","page":"Context & Device Management","title":"device() -> ZeDevice","text":"Get the current Level Zero device for the calling task. If no device has been explicitly set with device!, returns the first available device for the current driver. The device selection is task-local.","category":"section"},{"location":"api/context/#device!(dev::ZeDevice)-/-device!(i::Int)","page":"Context & Device Management","title":"device!(dev::ZeDevice) / device!(i::Int)","text":"Set the current Level Zero device for the calling task. Can pass either a device object or a 1-based device index.","category":"section"},{"location":"api/context/#devices()-Vector{ZeDevice}-/-devices(drv::ZeDriver)","page":"Context & Device Management","title":"devices() -> Vector{ZeDevice} / devices(drv::ZeDriver)","text":"Return a list of available Level Zero devices. Without arguments, returns devices for the current driver.","category":"section"},{"location":"api/context/#Context-Management","page":"Context & Device Management","title":"Context Management","text":"","category":"section"},{"location":"api/context/#context()-ZeContext","page":"Context & Device Management","title":"context() -> ZeContext","text":"Get the current Level Zero context for the calling task. If no context has been explicitly set with context!, returns a global context for the current driver. Contexts manage the lifetime of resources like memory allocations and command queues.","category":"section"},{"location":"api/context/#context!(ctx::ZeContext)","page":"Context & Device Management","title":"context!(ctx::ZeContext)","text":"Set the current Level Zero context for the calling task.","category":"section"},{"location":"api/context/#Command-Queues","page":"Context & Device Management","title":"Command Queues","text":"","category":"section"},{"location":"api/context/#global_queue(ctx::ZeContext,-dev::ZeDevice)-ZeCommandQueue","page":"Context & Device Management","title":"global_queue(ctx::ZeContext, dev::ZeDevice) -> ZeCommandQueue","text":"Get the global command queue for the given context and device. This queue is used as the default queue for executing operations. The queue is created with in-order execution flags.","category":"section"},{"location":"api/context/#synchronize()","page":"Context & Device Management","title":"synchronize()","text":"Block the host thread until all operations on the global command queue for the current context and device have completed.","category":"section"},{"location":"api/context/#Example-Workflow","page":"Context & Device Management","title":"Example Workflow","text":"using oneAPI\n\n# List available drivers\ndrv_list = drivers()\nprintln(\"Available drivers: \", length(drv_list))\n\n# Select a specific driver\ndriver!(drv_list[1])\n\n# List devices for current driver\ndev_list = devices()\nprintln(\"Available devices: \", length(dev_list))\n\n# Select a specific device\ndevice!(dev_list[1])\n\n# Get the current context (created automatically)\nctx = context()\n\n# Perform GPU operations...\na = oneArray(rand(Float32, 100))\n\n# Wait for all operations to complete\nsynchronize()","category":"section"},{"location":"api/context/#Multi-Device-Programming","page":"Context & Device Management","title":"Multi-Device Programming","text":"You can use different devices in different Julia tasks:\n\nusing oneAPI\n\n# Task 1: Use first device\nThreads.@spawn begin\n    device!(1)\n    a = oneArray(rand(Float32, 100))\n    # ... operations on device 1 ...\nend\n\n# Task 2: Use second device\nThreads.@spawn begin\n    device!(2)\n    b = oneArray(rand(Float32, 100))\n    # ... operations on device 2 ...\nend","category":"section"},{"location":"device/#Device-Intrinsics","page":"Device Intrinsics","title":"Device Intrinsics","text":"When writing custom kernels, you have access to a set of device intrinsics that map to underlying hardware instructions.","category":"section"},{"location":"device/#Indexing","page":"Device Intrinsics","title":"Indexing","text":"These functions allow you to determine the current thread's position in the execution grid.\n\nget_global_id(dim=0): Global index of the work item.\nget_local_id(dim=0): Local index of the work item within the workgroup.\nget_group_id(dim=0): Index of the workgroup.\nget_global_size(dim=0): Global size of the ND-range.\nget_local_size(dim=0): Size of the workgroup.\nget_num_groups(dim=0): Number of workgroups.","category":"section"},{"location":"device/#Synchronization","page":"Device Intrinsics","title":"Synchronization","text":"barrier(flags=0): Synchronizes all work items in a workgroup.","category":"section"},{"location":"device/#Atomics","page":"Device Intrinsics","title":"Atomics","text":"Atomic operations are supported for thread-safe updates to memory.\n\natomic_add!(ptr, val)\natomic_sub!(ptr, val)\natomic_inc!(ptr)\natomic_dec!(ptr)\natomic_min!(ptr, val)\natomic_max!(ptr, val)\natomic_and!(ptr, val)\natomic_or!(ptr, val)\natomic_xor!(ptr, val)\natomic_cmpxchg!(ptr, cmp, val)\n\nSupported types for atomics generally include Int32, Int64, UInt32, UInt64, Float32, and Float64.","category":"section"},{"location":"device/#Math-Functions","page":"Device Intrinsics","title":"Math Functions","text":"Standard math functions from Julia's Base are supported within kernels (e.g., sin, cos, exp, sqrt).","category":"section"},{"location":"kernels/#Kernel-Programming","page":"Kernel Programming","title":"Kernel Programming","text":"For maximum performance or custom operations not covered by high-level array abstractions, you can write custom kernels in Julia that execute on the GPU.","category":"section"},{"location":"kernels/#The-@oneapi-Macro","page":"Kernel Programming","title":"The @oneapi Macro","text":"The @oneapi macro is used to launch a kernel on the device. It takes configuration arguments like the number of items (threads) and groups (blocks).\n\nusing oneAPI\n\nfunction kernel(a, b)\n    i = get_global_id()\n    if i <= length(a)\n        @inbounds a[i] += b[i]\n    end\n    return\nend\n\na = oneArray(rand(Float32, 100))\nb = oneArray(rand(Float32, 100))\n\n# Launch configuration\nitems = 100\ngroups = 1\n\n@oneapi items=items groups=groups kernel(a, b)","category":"section"},{"location":"kernels/#KernelAbstractions.jl","page":"Kernel Programming","title":"KernelAbstractions.jl","text":"For portable kernel programming, it is highly recommended to use KernelAbstractions.jl. This allows you to write kernels that work on CPU, CUDA, ROCm, and oneAPI.\n\nusing KernelAbstractions, oneAPI\n\n@kernel function my_kernel!(a, b)\n    i = @index(Global, Linear)\n    @inbounds a[i] += b[i]\nend\n\n# Get the backend\nbackend = get_backend(a)\n\n# Instantiate the kernel\nk = my_kernel!(backend)\n\n# Launch with configuration\nk(a, b; ndrange=length(a))","category":"section"},{"location":"kernels/#Device-Intrinsics","page":"Kernel Programming","title":"Device Intrinsics","text":"Inside a kernel, you can use various intrinsics to interact with the hardware:\n\nget_global_id(): Get the global thread ID.\nget_local_id(): Get the local thread ID within a workgroup.\nget_group_id(): Get the workgroup ID.\nbarrier(): Synchronize threads within a workgroup.\n\nThese correspond to standard OpenCL/Level Zero intrinsics.","category":"section"},{"location":"api/#API-Reference","page":"Overview","title":"API Reference","text":"This page provides an overview of the oneAPI.jl API. For detailed documentation, see the specific API reference pages:\n\nContext & Device Management - Managing drivers, devices, and contexts\nArray Operations - Working with GPU arrays\nKernel Programming - Writing and launching custom kernels\nMemory Management - Memory allocation and transfer\nCompiler & Reflection - Code generation and introspection","category":"section"},{"location":"api/#Core-Functions","page":"Overview","title":"Core Functions","text":"","category":"section"},{"location":"api/#Compiler-Functions","page":"Overview","title":"Compiler Functions","text":"","category":"section"},{"location":"api/#oneL0-(Level-Zero)","page":"Overview","title":"oneL0 (Level Zero)","text":"Low-level bindings to the Level Zero API. See the Level Zero page for details.","category":"section"},{"location":"api/#oneMKL","page":"Overview","title":"oneMKL","text":"Intel oneAPI Math Kernel Library bindings. See the oneMKL page for details.","category":"section"},{"location":"api/#oneAPI.context!-Tuple{oneAPI.oneL0.ZeContext}","page":"Overview","title":"oneAPI.context!","text":"context!(ctx::ZeContext)\n\nSet the current Level Zero context for the calling task.\n\nThe context selection is task-local, allowing different Julia tasks to use different contexts.\n\nArguments\n\nctx::ZeContext: The context to use for subsequent operations.\n\nExamples\n\nctx = ZeContext(driver())\ncontext!(ctx)\n\nSee also: context, ZeContext\n\n\n\n\n\n","category":"method"},{"location":"api/#oneAPI.context-Tuple{}","page":"Overview","title":"oneAPI.context","text":"context() -> ZeContext\n\nGet the current Level Zero context for the calling task. If no context has been explicitly set with context!, returns a global context for the current driver.\n\nContexts manage the lifetime of resources like memory allocations and command queues. The context selection is task-local, but contexts themselves are cached globally per driver.\n\nExamples\n\nctx = context()\nprintln(\"Using context: \", ctx)\n\nSee also: context!, driver\n\n\n\n\n\n","category":"method"},{"location":"api/#oneAPI.device!-Tuple{oneAPI.oneL0.ZeDevice}","page":"Overview","title":"oneAPI.device!","text":"device!(dev::ZeDevice)\ndevice!(i::Int)\n\nSet the current Level Zero device for the calling task.\n\nThe device selection is task-local, allowing different Julia tasks to use different devices.\n\nArguments\n\ndev::ZeDevice: The device to use for subsequent operations.\ni::Int: Device index (1-based) from the list of available devices for the current driver.\n\nExamples\n\n# Select by device object\ndev = devices()[2]\ndevice!(dev)\n\n# Select by index\ndevice!(2)  # Select second device\n\nSee also: device, devices\n\n\n\n\n\n","category":"method"},{"location":"api/#oneAPI.device-Tuple{}","page":"Overview","title":"oneAPI.device","text":"device() -> ZeDevice\n\nGet the current Level Zero device for the calling task. If no device has been explicitly set with device!, returns the first available device for the current driver.\n\nThe device selection is task-local, allowing different Julia tasks to use different devices.\n\nExamples\n\ndev = device()\nprintln(\"Using device: \", dev)\n\nSee also: device!, devices, driver\n\n\n\n\n\n","category":"method"},{"location":"api/#oneAPI.driver!-Tuple{oneAPI.oneL0.ZeDriver}","page":"Overview","title":"oneAPI.driver!","text":"driver!(drv::ZeDriver)\n\nSet the current Level Zero driver for the calling task. This also clears the current device selection, as devices are associated with specific drivers.\n\nThe driver selection is task-local, allowing different Julia tasks to use different drivers.\n\nArguments\n\ndrv::ZeDriver: The driver to use for subsequent operations.\n\nExamples\n\ndrv = drivers()[2]  # Select second available driver\ndriver!(drv)\n\nSee also: driver, drivers\n\n\n\n\n\n","category":"method"},{"location":"api/#oneAPI.driver-Tuple{}","page":"Overview","title":"oneAPI.driver","text":"driver() -> ZeDriver\n\nGet the current Level Zero driver for the calling task. If no driver has been explicitly set with driver!, returns the first available driver.\n\nThe driver selection is task-local, allowing different Julia tasks to use different drivers.\n\nExamples\n\ndrv = driver()\nprintln(\"Using driver: \", drv)\n\nSee also: driver!, drivers\n\n\n\n\n\n","category":"method"},{"location":"api/#oneAPI.global_queue-Tuple{oneAPI.oneL0.ZeContext, oneAPI.oneL0.ZeDevice}","page":"Overview","title":"oneAPI.global_queue","text":"global_queue(ctx::ZeContext, dev::ZeDevice) -> ZeCommandQueue\n\nGet the global command queue for the given context and device. This queue is used as the default queue for executing operations, guaranteeing expected semantics when using a device on a Julia task.\n\nThe queue is created with in-order execution flags, meaning commands are executed in the order they are submitted. Queues are cached per task and (context, device) pair.\n\nArguments\n\nctx::ZeContext: The context for the command queue.\ndev::ZeDevice: The device for the command queue.\n\nReturns\n\nZeCommandQueue: A cached command queue with in-order execution.\n\nExamples\n\nctx = context()\ndev = device()\nqueue = global_queue(ctx, dev)\n\nSee also: context, device, synchronize\n\n\n\n\n\n","category":"method"},{"location":"api/#oneAPI.is_integrated","page":"Overview","title":"oneAPI.is_integrated","text":"is_integrated(dev::ZeDevice=device()) -> Bool\n\nCheck if the given device is an integrated GPU (i.e., integrated with the host processor).\n\nIntegrated GPUs share memory with the CPU and are typically found in laptop and desktop processors with integrated graphics.\n\nArguments\n\ndev::ZeDevice: The device to check. Defaults to the current device.\n\nReturns\n\ntrue if the device is integrated, false otherwise (e.g., discrete GPU).\n\nExamples\n\nif is_integrated()\n    println(\"Running on integrated graphics\")\nelse\n    println(\"Running on discrete GPU\")\nend\n\n# Check a specific device\ndev = devices()[1]\nis_integrated(dev)\n\nSee also: device, devices\n\n\n\n\n\n","category":"function"},{"location":"api/#oneAPI.oneL0.devices-Tuple{}","page":"Overview","title":"oneAPI.oneL0.devices","text":"devices() -> Vector{ZeDevice}\ndevices(drv::ZeDriver) -> Vector{ZeDevice}\n\nReturn a list of available Level Zero devices. Without arguments, returns devices for the current driver. With a driver argument, returns devices for that specific driver.\n\nExamples\n\n# Get devices for current driver\ndevs = devices()\nprintln(\"Found \", length(devs), \" devices\")\n\n# Get devices for specific driver\ndrv = drivers()[1]\ndevs = devices(drv)\n\nSee also: device, device!, drivers\n\n\n\n\n\n","category":"method"},{"location":"api/#oneAPI.@sync-Tuple{Any}","page":"Overview","title":"oneAPI.@sync","text":"@sync ex\n\nRun expression ex and synchronize the GPU afterwards.\n\nSee also: synchronize.\n\n\n\n\n\n","category":"macro"},{"location":"api/#oneAPI.kernel_convert-Tuple{Any}","page":"Overview","title":"oneAPI.kernel_convert","text":"kernel_convert(x)\n\nThis function is called for every argument to be passed to a kernel, allowing it to be converted to a GPU-friendly format. By default, the function does nothing and returns the input object x as-is.\n\nDo not add methods to this function, but instead extend the underlying Adapt.jl package and register methods for the the oneAPI.KernelAdaptor type.\n\n\n\n\n\n","category":"method"},{"location":"api/#oneAPI.@oneapi-Tuple","page":"Overview","title":"oneAPI.@oneapi","text":"@oneapi [kwargs...] kernel(args...)\n\nHigh-level interface for launching Julia kernels on Intel GPUs using oneAPI.\n\nThis macro compiles a Julia function to SPIR-V, prepares the arguments, and optionally launches the kernel on the GPU.\n\nKeyword Arguments\n\nMacro Keywords (compile-time)\n\nlaunch::Bool=true: Whether to launch the kernel immediately. If false, returns the compiled kernel object without executing it.\n\nCompiler Keywords\n\nkernel::Bool=false: Whether to compile as a kernel (true) or device function (false)\nname::Union{String,Nothing}=nothing: Explicit name for the kernel\nalways_inline::Bool=false: Whether to always inline device functions\n\nLaunch Keywords (runtime)\n\ngroups: Number of workgroups (required). Can be an integer or tuple.\nitems: Number of work-items per workgroup (required). Can be an integer or tuple.\nqueue::ZeCommandQueue=global_queue(...): Command queue to submit to.\n\nExamples\n\n# Simple vector addition kernel\nfunction vadd(a, b, c)\n    i = get_global_id()\n    @inbounds c[i] = a[i] + b[i]\n    return\nend\n\na = oneArray(rand(Float32, 1024))\nb = oneArray(rand(Float32, 1024))\nc = similar(a)\n\n# Launch with 4 workgroups of 256 items each\n@oneapi groups=4 items=256 vadd(a, b, c)\n\n# Compile without launching\nkernel = @oneapi launch=false vadd(a, b, c)\nkernel(a, b, c; groups=4, items=256)  # Launch later\n\nSee also: zefunction, kernel_convert\n\n\n\n\n\n","category":"macro"},{"location":"api/#oneAPI.return_type-Tuple{Any, Any}","page":"Overview","title":"oneAPI.return_type","text":"return_type(f, tt) -> r::Type\n\nReturn a type r such that f(args...)::r where args::tt.\n\n\n\n\n\n","category":"method"},{"location":"api/#oneAPI.oneL0.DeviceBuffer","page":"Overview","title":"oneAPI.oneL0.DeviceBuffer","text":"DeviceBuffer\n\nA buffer of device memory, owned by a specific device. Generally, may only be accessed by the device that owns it.\n\n\n\n\n\n","category":"type"},{"location":"api/#oneAPI.oneL0.HostBuffer","page":"Overview","title":"oneAPI.oneL0.HostBuffer","text":"HostBuffer\n\nA buffer of memory on the host. May be accessed by the host, and all devices within the host driver. Frequently used as staging areas to transfer data to or from devices.\n\nNote that these buffers need to be made resident to the device, e.g., by using the ZEKERNELFLAGFORCERESIDENCY module flag, the ZEKERNELSETATTRINDIRECTHOSTACCESS kernel attribute, or by calling zeDeviceMakeMemoryResident.\n\n\n\n\n\n","category":"type"},{"location":"api/#oneAPI.oneL0.OutOfGPUMemoryError","page":"Overview","title":"oneAPI.oneL0.OutOfGPUMemoryError","text":"OutOfGPUMemoryError(sz::Integer=0, dev::ZeDevice)\n\nAn operation allocated too much GPU memory.\n\n\n\n\n\n","category":"type"},{"location":"api/#oneAPI.oneL0.PtrOrZePtr","page":"Overview","title":"oneAPI.oneL0.PtrOrZePtr","text":"PtrOrZePtr{T}\n\nA special pointer type, ABI-compatible with both Ptr and ZePtr, for use in ccall expressions to convert values to either a device or a host type (in that order). This is required for APIs which accept pointers that either point to host or device memory.\n\n\n\n\n\n","category":"type"},{"location":"api/#oneAPI.oneL0.SharedBuffer","page":"Overview","title":"oneAPI.oneL0.SharedBuffer","text":"SharedBuffer\n\nA managed buffer that is shared between the host and one or more devices.\n\n\n\n\n\n","category":"type"},{"location":"api/#oneAPI.oneL0.ZeCommandList-Tuple{Union{Function, Type}, Vararg{Any}}","page":"Overview","title":"oneAPI.oneL0.ZeCommandList","text":"ZeCommandList(dev::ZeDevice, ...) do list\n    append_...!(list)\nend\n\nCreate a command list for device dev, passing in a do block that appends operations. The list is then closed and can be used immediately, e.g. for execution.\n\n\n\n\n\n","category":"method"},{"location":"api/#oneAPI.oneL0.ZeDim3","page":"Overview","title":"oneAPI.oneL0.ZeDim3","text":"ZeDim3(x)\n\nZeDim3((x,))\nZeDim3((x, y))\nZeDim3((x, y, x))\n\nA type used to specify dimensions, consisting of 3 integers for respectively the x, y and z dimension. Unspecified dimensions default to 1.\n\nOften accepted as argument through the ZeDim type alias, allowing to pass dimensions as a plain integer or a tuple without having to construct an explicit ZeDim3 object.\n\n\n\n\n\n","category":"type"},{"location":"api/#oneAPI.oneL0.ZePtr","page":"Overview","title":"oneAPI.oneL0.ZePtr","text":"ZePtr{T}\n\nA memory address that refers to data of type T that is accessible from q device. A ZePtr is ABI compatible with regular Ptr objects, e.g. it can be used to ccall a function that expects a Ptr to device memory, but it prevents erroneous conversions between the two.\n\n\n\n\n\n","category":"type"},{"location":"api/#oneAPI.oneL0.ZE_MAKE_VERSION-Tuple{Integer, Integer}","page":"Overview","title":"oneAPI.oneL0.ZE_MAKE_VERSION","text":"ze_make_version(major::Integer, minor::Integer) -> UInt32\n\n32-bit unsigned integer version number from major and minor components. This should be the Julia equivalent of the C macro: #define ZE_MAKE_VERSION( _major, _minor )  (( _major << 16 )|( _minor & 0x0000ffff))\n\n\n\n\n\n","category":"method"},{"location":"api/#oneAPI.oneL0.execute!","page":"Overview","title":"oneAPI.oneL0.execute!","text":"execute!(queue::ZeCommandQueue, ...) do list\n    append_...!(list)\nend\n\nCreate a command list for the device that owns queue, passing in a do block that appends operations. The list is then closed and executed on the queue.\n\n\n\n\n\n","category":"function"},{"location":"troubleshooting/#Troubleshooting","page":"Troubleshooting","title":"Troubleshooting","text":"","category":"section"},{"location":"troubleshooting/#Common-Issues","page":"Troubleshooting","title":"Common Issues","text":"","category":"section"},{"location":"troubleshooting/#No-devices-detected","page":"Troubleshooting","title":"No devices detected","text":"Symptom: oneAPI.devices() returns an empty list.\n\nSolution:\n\nEnsure you are running on Linux (recommended) or WSL2.\nCheck if the Intel Compute Runtime is installed and accessible.\nVerify your user has permissions to access the GPU render device (usually render group).\nRun oneAPI.versioninfo() to see detailed diagnostic information.","category":"section"},{"location":"troubleshooting/#\"Double-type-is-not-supported\"","page":"Troubleshooting","title":"\"Double type is not supported\"","text":"Symptom: Kernel compilation fails with an error about Float64 or Double support.\n\nSolution: Some Intel GPUs (especially integrated graphics) lack native hardware support for 64-bit floating point operations.\n\nUse Float32 instead of Float64.\nCheck support with:\nusing oneAPI.oneL0\noneL0.module_properties(device()).fp64flags & oneL0.ZE_DEVICE_MODULE_FLAG_FP64 != 0","category":"section"},{"location":"troubleshooting/#\"Out-of-memory\"-errors","page":"Troubleshooting","title":"\"Out of memory\" errors","text":"Symptom: Memory allocation fails.\n\nSolution:\n\nTrigger garbage collection: GC.gc().\nManually free unused arrays: oneAPI.unsafe_free!(array).\nCheck if you are exceeding the device's memory capacity.","category":"section"},{"location":"troubleshooting/#Debugging","page":"Troubleshooting","title":"Debugging","text":"","category":"section"},{"location":"troubleshooting/#Validation-Layer","page":"Troubleshooting","title":"Validation Layer","text":"Enable the Level Zero validation layer to catch API misuse:\n\nexport ZE_ENABLE_VALIDATION_LAYER=1\nexport ZE_ENABLE_PARAMETER_VALIDATION=1","category":"section"},{"location":"troubleshooting/#Debug-Mode","page":"Troubleshooting","title":"Debug Mode","text":"Enable debug mode in oneAPI.jl to use debug builds of underlying toolchains (if available):\n\noneAPI.set_debug!(true)","category":"section"},{"location":"api/memory/#Memory-Management","page":"Memory Management","title":"Memory Management","text":"This page documents memory management in oneAPI.jl.","category":"section"},{"location":"api/memory/#Memory-Operations","page":"Memory Management","title":"Memory Operations","text":"","category":"section"},{"location":"api/memory/#Base.unsafe_copyto!(ctx::ZeContext,-dev::ZeDevice,-dst,-src,-N)","page":"Memory Management","title":"Base.unsafe_copyto!(ctx::ZeContext, dev::ZeDevice, dst, src, N)","text":"Low-level memory copy operation on the GPU. Copies N elements from src to dst using the specified context and device. Both src and dst can be either host pointers (Ptr) or device pointers (ZePtr).\n\nwarning: Warning\nThis is a low-level function. No bounds checking is performed. For safe array copying, use copyto! on oneArray objects instead.","category":"section"},{"location":"api/memory/#unsafe_fill!(ctx::ZeContext,-dev::ZeDevice,-ptr,-pattern,-N)","page":"Memory Management","title":"unsafe_fill!(ctx::ZeContext, dev::ZeDevice, ptr, pattern, N)","text":"Low-level memory fill operation on the GPU. Fills N elements at ptr with the given pattern using the specified context and device.\n\nwarning: Warning\nThis is a low-level function. For safe array operations, use fill! on oneArray objects instead.","category":"section"},{"location":"api/memory/#Memory-Types","page":"Memory Management","title":"Memory Types","text":"oneAPI supports three types of memory through Unified Shared Memory (USM):","category":"section"},{"location":"api/memory/#Device-Memory-(Default)","page":"Memory Management","title":"Device Memory (Default)","text":"Fastest GPU access, not directly accessible from CPU.\n\n# Create array in device memory (default)\na = oneArray{Float32}(undef, 1000)\n@assert is_device(a)\n\n# Or explicitly specify\nb = oneArray{Float32,1,oneL0.DeviceBuffer}(undef, 1000)\n\nAdvantages:\n\nFastest GPU access\nBest for compute-intensive operations\n\nDisadvantages:\n\nCannot directly access from CPU\nRequires explicit copy to/from CPU\n\nUse when: Data stays on GPU for multiple operations","category":"section"},{"location":"api/memory/#Shared-Memory","page":"Memory Management","title":"Shared Memory","text":"Accessible from both CPU and GPU with automatic migration.\n\n# Create array in shared memory\na = oneArray{Float32,1,oneL0.SharedBuffer}(undef, 1000)\n@assert is_shared(a)\n\n# Can access from CPU\na[1] = 42.0f0  # Automatic migration to CPU\nprintln(a[1])  # Read from CPU\n\n# Can use in GPU kernels\n@oneapi groups=1 items=1000 kernel(a)  # Automatic migration to GPU\n\nAdvantages:\n\nAccessible from both CPU and GPU\nUnified virtual addressing\nAutomatic migration\n\nDisadvantages:\n\nMigration overhead\nSlower than device memory for pure GPU work\n\nUse when: Frequent CPU-GPU data exchange needed","category":"section"},{"location":"api/memory/#Host-Memory","page":"Memory Management","title":"Host Memory","text":"CPU memory that's pinned and visible to GPU.\n\n# Create array in host memory\na = oneArray{Float32,1,oneL0.HostBuffer}(undef, 1000)\n@assert is_host(a)\n\n# Direct CPU access\na[1] = 42.0f0\n\n# Can be used by GPU (but slower than device memory)\n@oneapi groups=1 items=1000 kernel(a)\n\nAdvantages:\n\nDirect CPU access\nPinned memory (faster PCIe transfers)\nGood for staging\n\nDisadvantages:\n\nSlower GPU access than device memory\nUses pinned system memory (limited resource)\n\nUse when: Staging data for transfer, or CPU needs to write while GPU reads","category":"section"},{"location":"api/memory/#Memory-Type-Comparison","page":"Memory Management","title":"Memory Type Comparison","text":"Feature Device Shared Host\nCPU Access ❌ No ✅ Yes ✅ Yes\nGPU Performance ⭐⭐⭐ Fastest ⭐⭐ Good ⭐ Slower\nMigration Manual Automatic Manual\nUse Case Pure GPU Mixed CPU/GPU Staging","category":"section"},{"location":"api/memory/#Memory-Allocation-and-Deallocation","page":"Memory Management","title":"Memory Allocation and Deallocation","text":"","category":"section"},{"location":"api/memory/#Automatic-Management","page":"Memory Management","title":"Automatic Management","text":"Julia's garbage collector automatically manages oneArray memory:\n\nfunction allocate_and_compute()\n    a = oneArray(rand(Float32, 1000))\n    b = oneArray(rand(Float32, 1000))\n    c = a .+ b\n    return Array(c)  # Only c is copied back\n    # a and b will be garbage collected\nend\n\nresult = allocate_and_compute()\n# GPU memory for a and b is freed eventually","category":"section"},{"location":"api/memory/#Manual-Garbage-Collection","page":"Memory Management","title":"Manual Garbage Collection","text":"Force garbage collection to free GPU memory:\n\n# Allocate large arrays\na = oneArray(rand(Float32, 10_000_000))\nb = oneArray(rand(Float32, 10_000_000))\n\n# Clear references\na = nothing\nb = nothing\n\n# Force GC to reclaim GPU memory\nGC.gc()","category":"section"},{"location":"api/memory/#Explicit-Freeing","page":"Memory Management","title":"Explicit Freeing","text":"Immediately free GPU memory (use with caution):\n\na = oneArray(rand(Float32, 1000))\n# ... use a ...\n\n# Explicitly free (dangerous if still in use!)\nunsafe_free!(a)\n\n# a is now invalid - do not use!\n\nwarning: Warning\nOnly use unsafe_free! when you're certain the array is no longer needed, including by any pending GPU operations. Prefer letting the GC handle cleanup.","category":"section"},{"location":"api/memory/#Do-Block-Pattern","page":"Memory Management","title":"Do-Block Pattern","text":"Use do-blocks for automatic cleanup:\n\nresult = oneArray{Float32}(1000) do temp\n    # temp is automatically freed when block exits\n    temp .= 1.0f0\n    sum(temp)  # Result is returned\nend","category":"section"},{"location":"api/memory/#Memory-Pooling","page":"Memory Management","title":"Memory Pooling","text":"oneAPI.jl uses memory pooling to reduce allocation overhead:\n\nusing oneAPI\n\n# Allocations are pooled\nfor i in 1:100\n    a = oneArray(rand(Float32, 1000))\n    # ... use a ...\n    # Memory is returned to pool, not freed\nend\n\nThe pool automatically manages memory reuse, reducing allocation costs.","category":"section"},{"location":"api/memory/#Checking-Memory-Usage","page":"Memory Management","title":"Checking Memory Usage","text":"Query GPU memory info:\n\nusing oneAPI.oneL0\n\ndev = device()\nprops = memory_properties(dev)\n\nfor prop in props\n    println(\"Memory size: \", prop.totalSize ÷ (1024^3), \" GB\")\nend","category":"section"},{"location":"api/memory/#Out-of-Memory-Errors","page":"Memory Management","title":"Out of Memory Errors","text":"If you encounter out-of-memory errors:","category":"section"},{"location":"api/memory/#1.-Reduce-Batch-Size","page":"Memory Management","title":"1. Reduce Batch Size","text":"# Instead of processing all at once\nresult = process(oneArray(huge_data))\n\n# Process in smaller batches\nfor batch in batches(huge_data, size=1000)\n    result = process(oneArray(batch))\n    # Process result...\nend","category":"section"},{"location":"api/memory/#2.-Free-Unused-Arrays","page":"Memory Management","title":"2. Free Unused Arrays","text":"a = oneArray(rand(Float32, 1_000_000))\nb = compute(a)\n\n# If 'a' is no longer needed\nunsafe_free!(a)\n\n# Continue with 'b'\nresult = process(b)","category":"section"},{"location":"api/memory/#3.-Use-Shared-or-Host-Memory","page":"Memory Management","title":"3. Use Shared or Host Memory","text":"# Instead of device memory\na = oneArray{Float32}(undef, huge_size)\n\n# Use shared memory (can swap to system RAM)\na = oneArray{Float32,1,oneL0.SharedBuffer}(undef, huge_size)","category":"section"},{"location":"api/memory/#4.-Force-Garbage-Collection","page":"Memory Management","title":"4. Force Garbage Collection","text":"# After freeing references\nlarge_array = nothing\nGC.gc()  # Immediately reclaim GPU memory","category":"section"},{"location":"api/memory/#5.-Use-Multiple-Devices","page":"Memory Management","title":"5. Use Multiple Devices","text":"# Distribute work across devices\nfor (i, dev_id) in enumerate(1:length(devices()))\n    Threads.@spawn begin\n        device!(dev_id)\n        partition = data_partitions[i]\n        a = oneArray(partition)\n        result = compute(a)\n        # ...\n    end\nend","category":"section"},{"location":"api/memory/#Low-Level-Memory-Operations","page":"Memory Management","title":"Low-Level Memory Operations","text":"For advanced users, oneL0 provides direct memory management:\n\nusing oneAPI.oneL0\n\nctx = context()\ndev = device()\n\n# Allocate device memory\nptr = device_alloc(ctx, dev, 1024, 8)  # 1024 bytes, 8-byte aligned\n\n# Copy data\ndata = rand(Float32, 256)\nGC.@preserve data begin\n    unsafe_copyto!(ctx, dev, ptr, pointer(data), 256)\nend\n\n# Free memory\nfree(ctx, ptr)","category":"section"},{"location":"api/memory/#Memory-Advise-and-Prefetch","page":"Memory Management","title":"Memory Advise and Prefetch","text":"Hint to the runtime about memory usage (shared memory only):\n\nusing oneAPI.oneL0\n\na = oneArray{Float32,1,oneL0.SharedBuffer}(undef, 1000)\n\n# Advise that this will be read-only on the device\n# (Implementation depends on Level Zero driver support)\n\n# Prefetch to device\nctx = context()\ndev = device()\nqueue = global_queue(ctx, dev)\n\nexecute!(queue) do list\n    append_prefetch!(list, pointer(a), sizeof(a))\nend","category":"section"},{"location":"api/memory/#Best-Practices","page":"Memory Management","title":"Best Practices","text":"Use device memory by default for best GPU performance\nUse shared memory when you need CPU access without explicit copies\nUse host memory for staging data or when CPU writes frequently\nLet GC handle cleanup unless you have specific memory pressure\nReuse allocations within loops when possible\nProfile memory usage to identify bottlenecks\nBe cautious with unsafe_free! - use only when you're certain it's safe","category":"section"},{"location":"api/memory/#Example:-Efficient-Memory-Usage","page":"Memory Management","title":"Example: Efficient Memory Usage","text":"using oneAPI\n\nfunction efficient_pipeline(data_batches)\n    # Allocate output buffer once\n    result = oneArray{Float32}(undef, 1000)\n    results = Float32[]\n\n    for batch in data_batches\n        # Reuse input buffer by copying\n        input = oneArray(batch)\n\n        # Compute in-place when possible\n        @oneapi groups=4 items=250 process_kernel!(result, input)\n\n        # Copy result back\n        push!(results, Array(result)...)\n\n        # Input is freed when loop continues\n    end\n\n    return results\nend","category":"section"},{"location":"arrays/#Array-Programming","page":"Array Programming","title":"Array Programming","text":"oneAPI.jl provides an array type, oneArray, which lives on the GPU. It implements the interface defined by GPUArrays.jl, allowing for high-level array operations.","category":"section"},{"location":"arrays/#The-oneArray-Type","page":"Array Programming","title":"The oneArray Type","text":"The oneArray{T,N} type represents an N-dimensional array with elements of type T stored on the GPU.\n\nusing oneAPI\n\n# Allocate an uninitialized array\na = oneArray{Float32}(undef, 1024)\n\n# Initialize from a CPU array\nb = oneArray([1, 2, 3, 4])\n\n# Initialize with zeros/ones\nz = oneAPI.zeros(Float32, 100)\no = oneAPI.ones(Float32, 100)","category":"section"},{"location":"arrays/#Array-Operations","page":"Array Programming","title":"Array Operations","text":"Since oneArray implements the AbstractArray interface, you can use standard Julia array operations.\n\na = oneArray(rand(Float32, 10))\nb = oneArray(rand(Float32, 10))\n\nc = a .+ b        # Element-wise addition\nd = sum(a)        # Reduction\ne = map(sin, a)   # Map","category":"section"},{"location":"arrays/#Data-Transfer","page":"Array Programming","title":"Data Transfer","text":"To move data between the host (CPU) and the device (GPU), use the constructors or copyto!.\n\n# CPU to GPU\nd_a = oneArray(h_a)\n\n# GPU to CPU\nh_a = Array(d_a)","category":"section"},{"location":"arrays/#Backend-Agnostic-Programming","page":"Array Programming","title":"Backend Agnostic Programming","text":"To write code that works on both CPU and GPU (and other backends like CUDA), use the generic array interfaces provided by GPUArrays.jl. Avoid hardcoding oneArray in your functions; instead, accept AbstractArray and let the dispatch system handle the specific implementation.\n\nfunction generic_add!(a::AbstractArray, b::AbstractArray)\n    a .+= b\n    return a\nend\n\n# Works on CPU\ngeneric_add!(rand(10), rand(10))\n\n# Works on Intel GPU\ngeneric_add!(oneArray(rand(10)), oneArray(rand(10)))","category":"section"},{"location":"api/compiler/#Compiler-and-Reflection","page":"Compiler & Reflection","title":"Compiler and Reflection","text":"This page documents the compiler interface and code reflection tools for oneAPI.jl.","category":"section"},{"location":"api/compiler/#Code-Reflection","page":"Compiler & Reflection","title":"Code Reflection","text":"oneAPI.jl provides macros for inspecting code generation at various stages:\n\n@device_code_lowered - Show lowered IR (desugared Julia code)\n@device_code_typed - Show type-inferred IR\n@device_code_warntype - Show type-inferred IR with type stability warnings\n@device_code_llvm - Show LLVM IR\n@device_code_spirv - Show SPIR-V assembly\n@device_code - Show all compilation stages interactively\n\nThese macros are re-exported from GPUCompiler.jl. See the GPUCompiler documentation for detailed usage.","category":"section"},{"location":"api/compiler/#return_type(f,-tt)-Type","page":"Compiler & Reflection","title":"return_type(f, tt) -> Type","text":"Return the inferred return type of function f when called with argument types tt in a GPU kernel context.\n\nArguments:\n\nf: Function to analyze\ntt: Tuple type of arguments\n\nReturns:\n\nType that f(args...) would return where args::tt\n\nExample:\n\nfunction compute(x::Float32)\n    return x * 2.0f0\nend\n\nrt = oneAPI.return_type(compute, Tuple{Float32})\n@assert rt == Float32","category":"section"},{"location":"api/compiler/#Inspecting-Generated-Code","page":"Compiler & Reflection","title":"Inspecting Generated Code","text":"Code reflection tools help you understand how your Julia code is compiled to GPU code:","category":"section"},{"location":"api/compiler/#LLVM-IR","page":"Compiler & Reflection","title":"LLVM IR","text":"View the LLVM intermediate representation:\n\nusing oneAPI\n\nfunction kernel(a, b)\n    i = get_global_id()\n    @inbounds a[i] = b[i] + 1.0f0\n    return\nend\n\na = oneArray(zeros(Float32, 10))\nb = oneArray(rand(Float32, 10))\n\n@device_code_llvm @oneapi groups=1 items=10 kernel(a, b)","category":"section"},{"location":"api/compiler/#SPIR-V-Assembly","page":"Compiler & Reflection","title":"SPIR-V Assembly","text":"View the final SPIR-V assembly that runs on the GPU:\n\n@device_code_spirv @oneapi groups=1 items=10 kernel(a, b)","category":"section"},{"location":"api/compiler/#Type-Inference","page":"Compiler & Reflection","title":"Type Inference","text":"Check for type instabilities that hurt performance:\n\n@device_code_warntype @oneapi groups=1 items=10 kernel(a, b)","category":"section"},{"location":"api/compiler/#Type-Inferred-IR","page":"Compiler & Reflection","title":"Type-Inferred IR","text":"See the typed intermediate representation:\n\n@device_code_typed @oneapi groups=1 items=10 kernel(a, b)","category":"section"},{"location":"api/compiler/#Interactive-Inspection","page":"Compiler & Reflection","title":"Interactive Inspection","text":"Use @device_code for an interactive menu:\n\n@device_code @oneapi groups=1 items=10 kernel(a, b)\n# Opens a menu to select which compilation stage to view","category":"section"},{"location":"api/compiler/#Return-Type-Inference","page":"Compiler & Reflection","title":"Return Type Inference","text":"Query the return type of a kernel:\n\nfunction compute(x::Float32)\n    return x * 2.0f0\nend\n\n# Infer return type\nrt = oneAPI.return_type(compute, Tuple{Float32})\n@assert rt == Float32","category":"section"},{"location":"api/compiler/#Debugging-Type-Issues","page":"Compiler & Reflection","title":"Debugging Type Issues","text":"","category":"section"},{"location":"api/compiler/#Common-Type-Instability-Sources","page":"Compiler & Reflection","title":"Common Type Instability Sources","text":"# ❌ Type instability: Conditional returns different types\nfunction bad_kernel(x, flag)\n    if flag\n        return x        # Float32\n    else\n        return 0        # Int\n    end\nend\n\n# ✅ Type stable: Consistent return type\nfunction good_kernel(x, flag)\n    if flag\n        return x        # Float32\n    else\n        return 0.0f0    # Float32\n    end\nend","category":"section"},{"location":"api/compiler/#Using-@device*code*warntype","page":"Compiler & Reflection","title":"Using @devicecodewarntype","text":"function mystery_kernel!(output, input)\n    i = get_global_id()\n    @inbounds output[i] = some_complex_function(input[i])\n    return\nend\n\n# Check for type issues\n@device_code_warntype @oneapi groups=1 items=10 mystery_kernel!(a, b)\n\n# Look for red warnings indicating type instability","category":"section"},{"location":"api/compiler/#Compilation-Options","page":"Compiler & Reflection","title":"Compilation Options","text":"","category":"section"},{"location":"api/compiler/#Kernel-vs-Device-Function","page":"Compiler & Reflection","title":"Kernel vs Device Function","text":"# Compile as kernel (default for @oneapi)\n@device_code_llvm @oneapi kernel=true kernel(a, b)\n\n# Compile as device function (callable from other kernels)\n@device_code_llvm @oneapi kernel=false helper_function(x)","category":"section"},{"location":"api/compiler/#Always-Inline","page":"Compiler & Reflection","title":"Always Inline","text":"Force inlining of device functions:\n\n@oneapi always_inline=true kernel(a, b)","category":"section"},{"location":"api/compiler/#Custom-Kernel-Name","page":"Compiler & Reflection","title":"Custom Kernel Name","text":"Specify a custom name for the kernel:\n\n@oneapi name=\"my_custom_kernel\" kernel(a, b)","category":"section"},{"location":"api/compiler/#Example:-Optimizing-a-Kernel","page":"Compiler & Reflection","title":"Example: Optimizing a Kernel","text":"Here's a workflow for optimizing a kernel using reflection tools:\n\nusing oneAPI\n\n# Initial version\nfunction sum_kernel_v1!(result, data)\n    i = get_global_id()\n    if i == 1\n        sum = 0\n        for j in 1:length(data)\n            sum += data[j]\n        end\n        result[1] = sum\n    end\n    return\nend\n\ndata = oneArray(rand(Float32, 1000))\nresult = oneArray(zeros(Float32, 1))\n\n# Check for type issues\n@device_code_warntype @oneapi groups=1 items=1 sum_kernel_v1!(result, data)\n# Notice: `sum` might be Int instead of Float32!\n\n# Fixed version\nfunction sum_kernel_v2!(result, data)\n    i = get_global_id()\n    if i == 1\n        sum = 0.0f0  # Explicitly Float32\n        for j in 1:length(data)\n            sum += data[j]\n        end\n        result[1] = sum\n    end\n    return\nend\n\n# Verify the fix\n@device_code_warntype @oneapi groups=1 items=1 sum_kernel_v2!(result, data)\n# Should be type-stable now!\n\n# Check the generated code\n@device_code_llvm @oneapi groups=1 items=1 sum_kernel_v2!(result, data)","category":"section"},{"location":"api/compiler/#Profiling","page":"Compiler & Reflection","title":"Profiling","text":"For performance profiling, see the Performance Guide.","category":"section"},{"location":"api/compiler/#Troubleshooting","page":"Compiler & Reflection","title":"Troubleshooting","text":"","category":"section"},{"location":"api/compiler/#Compilation-Errors","page":"Compiler & Reflection","title":"Compilation Errors","text":"If you encounter compilation errors:\n\nCheck type stability: Use @device_code_warntype\nInspect LLVM IR: Use @device_code_llvm to see if the issue is in LLVM generation\nSimplify the kernel: Comment out sections to isolate the problematic code\nCheck argument types: Ensure arguments are GPU-compatible (isbits types)","category":"section"},{"location":"api/compiler/#SPIR-V-Issues","page":"Compiler & Reflection","title":"SPIR-V Issues","text":"If SPIR-V generation fails:\n\nUpdate dependencies: Ensure SPIRV-LLVM-Translator is up to date\nCheck device capabilities: Some operations require specific hardware features\nReduce complexity: Very complex kernels might hit compiler limits","category":"section"},{"location":"api/compiler/#Performance-Issues","page":"Compiler & Reflection","title":"Performance Issues","text":"If your kernel is slow:\n\nProfile memory access patterns: Coalesced access is crucial\nCheck occupancy: Are you launching enough work-items?\nMinimize barriers: Synchronization has overhead\nUse local memory wisely: It's faster than global memory but limited in size","category":"section"},{"location":"api/kernels/#Kernel-Programming","page":"Kernel Programming","title":"Kernel Programming","text":"This page documents the kernel programming API for writing custom GPU kernels in oneAPI.jl.","category":"section"},{"location":"api/kernels/#Kernel-Launch","page":"Kernel Programming","title":"Kernel Launch","text":"","category":"section"},{"location":"api/kernels/#@oneapi-[kwargs...]-kernel(args...)","page":"Kernel Programming","title":"@oneapi [kwargs...] kernel(args...)","text":"High-level interface for launching Julia kernels on Intel GPUs using oneAPI.\n\nThis macro compiles a Julia function to SPIR-V, prepares the arguments, and optionally launches the kernel on the GPU.\n\nKeyword Arguments:\n\nMacro Keywords (compile-time):\n\nlaunch::Bool=true: Whether to launch the kernel immediately\n\nCompiler Keywords:\n\nkernel::Bool=false: Whether to compile as a kernel or device function\nname::Union{String,Nothing}=nothing: Explicit name for the kernel\nalways_inline::Bool=false: Whether to always inline device functions\n\nLaunch Keywords (runtime):\n\ngroups: Number of workgroups (required). Can be an integer or tuple.\nitems: Number of work-items per workgroup (required). Can be an integer or tuple.\nqueue::ZeCommandQueue: Command queue to submit to (defaults to global queue).","category":"section"},{"location":"api/kernels/#zefunction(f,-tt;-kwargs...)","page":"Kernel Programming","title":"zefunction(f, tt; kwargs...)","text":"Compile a Julia function to a Level Zero kernel function. This is the lower-level interface used by @oneapi. Returns a callable kernel object.","category":"section"},{"location":"api/kernels/#kernel_convert(x)","page":"Kernel Programming","title":"kernel_convert(x)","text":"Convert arguments for kernel execution. This function is called for every argument passed to a kernel, allowing customization of argument conversion. By default, it converts oneArray to oneDeviceArray.","category":"section"},{"location":"api/kernels/#Basic-Kernel-Example","page":"Kernel Programming","title":"Basic Kernel Example","text":"using oneAPI\n\nfunction vadd_kernel!(a, b, c)\n    i = get_global_id()\n    if i <= length(a)\n        @inbounds c[i] = a[i] + b[i]\n    end\n    return\nend\n\nN = 1024\na = oneArray(rand(Float32, N))\nb = oneArray(rand(Float32, N))\nc = similar(a)\n\n# Launch with 4 workgroups of 256 work-items each\n@oneapi groups=4 items=256 vadd_kernel!(a, b, c)","category":"section"},{"location":"api/kernels/#Launch-Configuration","page":"Kernel Programming","title":"Launch Configuration","text":"","category":"section"},{"location":"api/kernels/#Workgroups-and-Work-Items","page":"Kernel Programming","title":"Workgroups and Work-Items","text":"The oneAPI execution model is based on:\n\nWork-items: Individual threads of execution (analogous to CUDA threads)\nWorkgroups: Groups of work-items that can synchronize and share local memory (analogous to CUDA blocks)\n\n# 1D configuration\n@oneapi groups=10 items=64 kernel(args...)        # 640 work-items total\n\n# 2D configuration\n@oneapi groups=(10, 10) items=(8, 8) kernel(args...)  # 6400 work-items total\n\n# 3D configuration\n@oneapi groups=(4, 4, 4) items=(4, 4, 4) kernel(args...)  # 4096 work-items total","category":"section"},{"location":"api/kernels/#Determining-Launch-Configuration","page":"Kernel Programming","title":"Determining Launch Configuration","text":"# For simple element-wise operations\nN = length(array)\nitems = 256  # Typical workgroup size\ngroups = cld(N, items)  # Ceiling division\n\n@oneapi groups=groups items=items kernel(array)","category":"section"},{"location":"api/kernels/#Compile-Without-Launch","page":"Kernel Programming","title":"Compile Without Launch","text":"You can compile a kernel without launching it:\n\n# Compile the kernel\nkernel = @oneapi launch=false vadd_kernel!(a, b, c)\n\n# Launch later with different configurations\nkernel(a, b, c; groups=4, items=256)\nkernel(a, b, c; groups=8, items=128)","category":"section"},{"location":"api/kernels/#Device-Intrinsics","page":"Kernel Programming","title":"Device Intrinsics","text":"Inside GPU kernels, you can use various intrinsics to query execution context and synchronize work-items.","category":"section"},{"location":"api/kernels/#Thread-Indexing","page":"Kernel Programming","title":"Thread Indexing","text":"# Global ID (unique across all work-items)\ni = get_global_id()      # 1D linear index\ni = get_global_id(0)     # X dimension\nj = get_global_id(1)     # Y dimension\nk = get_global_id(2)     # Z dimension\n\n# Local ID (within workgroup)\nlocal_i = get_local_id()   # 1D linear index\nlocal_i = get_local_id(0)  # X dimension\nlocal_j = get_local_id(1)  # Y dimension\nlocal_k = get_local_id(2)  # Z dimension\n\n# Workgroup ID\ngroup_i = get_group_id(0)  # X dimension\ngroup_j = get_group_id(1)  # Y dimension\ngroup_k = get_group_id(2)  # Z dimension\n\n# Workgroup size\nlocal_size = get_local_size()   # Total work-items in workgroup\nlocal_size_x = get_local_size(0)\nlocal_size_y = get_local_size(1)\n\n# Global size\nglobal_size = get_global_size()   # Total work-items\nglobal_size_x = get_global_size(0)","category":"section"},{"location":"api/kernels/#2D-Matrix-Example","page":"Kernel Programming","title":"2D Matrix Example","text":"function matmul_kernel!(C, A, B)\n    # Get 2D indices\n    row = get_global_id(0)\n    col = get_global_id(1)\n\n    if row <= size(C, 1) && col <= size(C, 2)\n        sum = 0.0f0\n        for k in 1:size(A, 2)\n            @inbounds sum += A[row, k] * B[k, col]\n        end\n        @inbounds C[row, col] = sum\n    end\n    return\nend\n\nM, N, K = 256, 256, 256\nA = oneArray(rand(Float32, M, K))\nB = oneArray(rand(Float32, K, N))\nC = oneArray{Float32}(undef, M, N)\n\n# Launch with 2D configuration\nitems = (16, 16)  # 16x16 work-items per workgroup\ngroups = (cld(M, items[1]), cld(N, items[2]))\n\n@oneapi groups=groups items=items matmul_kernel!(C, A, B)","category":"section"},{"location":"api/kernels/#Synchronization","page":"Kernel Programming","title":"Synchronization","text":"# Barrier: synchronize all work-items in a workgroup\nbarrier()\n\n# Memory fences (ensure memory operations are visible)\nmem_fence()     # Both local and global memory\nlocal_mem_fence()   # Local memory only\nglobal_mem_fence()  # Global memory only","category":"section"},{"location":"api/kernels/#Local-Memory","page":"Kernel Programming","title":"Local Memory","text":"Local memory (workgroup-shared memory) enables cooperation between work-items:\n\nfunction optimized_reduction!(result, input)\n    local_id = get_local_id()\n    local_size = get_local_size()\n\n    # Allocate local memory (shared within workgroup)\n    local_data = oneLocalArray(Float32, 256)\n\n    # Load into local memory\n    @inbounds local_data[local_id] = input[get_global_id()]\n    barrier()\n\n    # Tree reduction in local memory\n    stride = local_size ÷ 2\n    while stride > 0\n        if local_id <= stride\n            @inbounds local_data[local_id] += local_data[local_id + stride]\n        end\n        barrier()\n        stride ÷= 2\n    end\n\n    # First work-item writes result\n    if local_id == 1\n        @inbounds result[get_group_id()] = local_data[1]\n    end\n    return\nend","category":"section"},{"location":"api/kernels/#Atomic-Operations","page":"Kernel Programming","title":"Atomic Operations","text":"For thread-safe operations on shared data:\n\n# Atomic add\noneAPI.atomic_add!(ptr, value)\n\n# Atomic exchange\nold_value = oneAPI.atomic_xchg!(ptr, new_value)\n\n# Atomic compare-and-swap\nold_value = oneAPI.atomic_cas!(ptr, compare, new_value)\n\n# Atomic min/max\noneAPI.atomic_min!(ptr, value)\noneAPI.atomic_max!(ptr, value)\n\nExample histogram kernel:\n\nfunction histogram_kernel!(hist, data, bins)\n    i = get_global_id()\n    if i <= length(data)\n        @inbounds val = data[i]\n        bin = clamp(floor(Int, val * bins) + 1, 1, bins)\n        oneAPI.atomic_add!(pointer(hist, bin), 1)\n    end\n    return\nend","category":"section"},{"location":"api/kernels/#Kernel-Restrictions","page":"Kernel Programming","title":"Kernel Restrictions","text":"GPU kernels have certain restrictions:\n\nMust return nothing: Kernels cannot return values directly. Use output arrays instead.\nNo dynamic memory allocation: Cannot allocate arrays inside kernels\nNo I/O operations: Cannot print or write to files (use printf-style debugging with care)\nLimited recursion: Avoid or minimize recursive calls\nType stability: Ensure type-stable code for best performance\n\n# ❌ Bad: Returns a value\nfunction bad_kernel(a)\n    return a[1] + 1\nend\n\n# ✅ Good: Returns nothing, uses output parameter\nfunction good_kernel!(result, a)\n    @inbounds result[1] = a[1] + 1\n    return\nend","category":"section"},{"location":"api/kernels/#KernelAbstractions.jl","page":"Kernel Programming","title":"KernelAbstractions.jl","text":"For portable GPU programming across CUDA, AMD, and Intel GPUs, use KernelAbstractions.jl:\n\nusing KernelAbstractions\nusing oneAPI\n\n@kernel function generic_kernel!(a, b)\n    i = @index(Global)\n    @inbounds a[i] = a[i] + b[i]\nend\n\na = oneArray(rand(Float32, 100))\nb = oneArray(rand(Float32, 100))\n\nbackend = get_backend(a)  # oneAPIBackend()\nkernel! = generic_kernel!(backend)\nkernel!(a, b, ndrange=length(a))\n\nSee the KernelAbstractions.jl documentation for more details.","category":"section"},{"location":"api/kernels/#Debugging-Kernels","page":"Kernel Programming","title":"Debugging Kernels","text":"See the Compiler and Reflection page for tools to inspect generated code and debug kernels.","category":"section"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/#Basic-Usage","page":"Getting Started","title":"Basic Usage","text":"The most basic usage involves moving data to the GPU using oneArray and performing operations on it.\n\nusing oneAPI\n\n# Create an array on the CPU\na = rand(Float32, 1024)\n\n# Move it to the GPU\nd_a = oneArray(a)\n\n# Perform operations on the GPU\nd_b = d_a .+ 1.0f0\n\n# Move the result back to the CPU\nb = Array(d_b)","category":"section"},{"location":"getting_started/#Matrix-Multiplication","page":"Getting Started","title":"Matrix Multiplication","text":"Matrix multiplication is accelerated using the oneMKL library when available.\n\nusing oneAPI\n\nA = oneArray(rand(Float32, 128, 128))\nB = oneArray(rand(Float32, 128, 128))\n\n# This operation runs on the GPU\nC = A * B","category":"section"},{"location":"getting_started/#Writing-Kernels","page":"Getting Started","title":"Writing Kernels","text":"For custom operations, you can write kernels using the @oneapi macro.\n\nusing oneAPI\n\nfunction my_kernel(a, b)\n    i = get_global_id()\n    @inbounds a[i] += b[i]\n    return\nend\n\na = oneArray(ones(Float32, 1024))\nb = oneArray(ones(Float32, 1024))\n\n# Launch the kernel with 1024 items\n@oneapi items=1024 my_kernel(a, b)\n\nSee the Kernel Programming section for more details.","category":"section"},{"location":"memory/#Memory-Management","page":"Memory Management","title":"Memory Management","text":"Efficient memory management is crucial for GPU programming. oneAPI.jl provides tools to manage device memory allocation and data transfer.","category":"section"},{"location":"memory/#Unified-Shared-Memory-(USM)","page":"Memory Management","title":"Unified Shared Memory (USM)","text":"oneAPI uses Unified Shared Memory, which allows for pointers that can be accessible from both the host and the device, or specific to one.\n\nDevice Memory: Accessible only by the device. Fastest access for kernels.\nHost Memory: Accessible by the host and device.\nShared Memory: Automatically migrated between host and device.\n\noneArray typically uses device memory for performance.","category":"section"},{"location":"memory/#Allocation","page":"Memory Management","title":"Allocation","text":"You can perform low-level memory allocation using the oneL0 submodule if needed, though oneArray handles this automatically.\n\nusing oneAPI.oneL0\n\n# Allocate device memory\nptr = oneL0.zeMemAllocDevice(context(), device(), 1024, 1)\n\n# Free memory\noneL0.zeMemFree(context(), ptr)","category":"section"},{"location":"memory/#Garbage-Collection","page":"Memory Management","title":"Garbage Collection","text":"Julia's garbage collector automatically manages oneArray objects. However, GPU memory is a limited resource. If you are running into out-of-memory errors, you might need to manually trigger garbage collection or free arrays.\n\na = oneArray(rand(Float32, 1024*1024*100))\na = nothing\nGC.gc() # Reclaim memory","category":"section"},{"location":"memory/#Explicit-Freeing","page":"Memory Management","title":"Explicit Freeing","text":"For immediate memory release, you can use unsafe_free!:\n\nusing oneAPI\n\na = oneArray(rand(1024))\noneAPI.unsafe_free!(a)\n\nWarning: Only use unsafe_free! if you are sure the array is no longer used, including by any pending GPU operations.","category":"section"},{"location":"#oneAPI.jl","page":"Home","title":"oneAPI.jl","text":"Julia support for the oneAPI programming toolkit.\n\noneAPI.jl provides support for working with the oneAPI unified programming model. The package is currently verified to work with the implementation provided by the Intel Compute Runtime, primarily on Linux.","category":"section"},{"location":"#Writing-Portable-Code","page":"Home","title":"Writing Portable Code","text":"While oneAPI.jl provides specific functionality for Intel GPUs, it is highly recommended to write backend-agnostic code whenever possible. This allows your code to run on various hardware backends (NVIDIA, AMD, Intel, Apple) without modification.\n\nGPUArrays.jl: Use high-level array abstractions that work across different GPU backends.\nKernelAbstractions.jl: Use this package for writing kernels that can be compiled for CPU, CUDA, ROCm, and oneAPI devices.\n\nDirect use of oneAPI-specific macros (like @oneapi) and types (like oneArray) should be reserved for cases where you need specific optimizations or features not covered by the generic abstractions.","category":"section"},{"location":"#Features","page":"Home","title":"Features","text":"High-level Array Abstractions: oneArray type fully implementing the GPUArrays.jl interface.\nKernel Programming: Execute custom kernels written in Julia on Intel GPUs.\nLevel Zero Integration: Low-level access to the Level Zero API via the oneL0 submodule.\noneMKL Support: Integration with Intel oneMKL for BLAS, LAPACK, and sparse operations.\nSYCL Integration: Interoperability with SYCL (on Linux).","category":"section"},{"location":"#Requirements","page":"Home","title":"Requirements","text":"Julia: 1.10 or higher\nOS: Linux\nHardware: Intel Gen9 graphics or newer (including Intel Arc A-Series)","category":"section"}]
}
